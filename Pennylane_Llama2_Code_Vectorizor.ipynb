{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBhHh1o6NNXJJYf4WsyPXg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graylan0/ModeZion/blob/main/Pennylane_Llama2_Code_Vectorizor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SKeUsmPkDmDh",
        "outputId": "63c02090-8c0b-4399-9a48-82104343c06b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.8.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.4)\n",
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.33.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.2.1)\n",
            "Collecting rustworkx (from pennylane)\n",
            "  Downloading rustworkx-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
            "Collecting semantic-version>=2.7 (from pennylane)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting autoray>=0.6.1 (from pennylane)\n",
            "  Downloading autoray-0.6.7-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.2)\n",
            "Collecting pennylane-lightning>=0.33 (from pennylane)\n",
            "  Downloading PennyLane_Lightning-0.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.5.0)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2023.7.22)\n",
            "Installing collected packages: semantic-version, rustworkx, autoray, pennylane-lightning, pennylane\n",
            "Successfully installed autoray-0.6.7 pennylane-0.33.1 pennylane-lightning-0.33.1 rustworkx-0.13.2 semantic-version-2.10.0\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n",
            "Collecting asyncio\n",
            "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: asyncio\n",
            "Successfully installed asyncio-3.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "asyncio"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install aiohttp\n",
        "!pip install pennylane\n",
        "!pip install textblob\n",
        "!pip install asyncio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WQtdIitDmhV",
        "outputId": "b9b03c5e-5791-4772-b52e-2aea42043c54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-18 11:19:03--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.178.118, 65.8.178.12, 65.8.178.93, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.178.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/30/e3/30e3aca7233f7337633262ff6d59dd98559ecd8982e7419b39752c8d0daae1ca/3bfdde943555c78294626a6ccd40184162d066d39774bd2c98dae24943d32cc3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.ggmlv3.q8_0.bin%3B+filename%3D%22llama-2-7b-chat.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1700565543&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMDU2NTU0M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMC9lMy8zMGUzYWNhNzIzM2Y3MzM3NjMzMjYyZmY2ZDU5ZGQ5ODU1OWVjZDg5ODJlNzQxOWIzOTc1MmM4ZDBkYWFlMWNhLzNiZmRkZTk0MzU1NWM3ODI5NDYyNmE2Y2NkNDAxODQxNjJkMDY2ZDM5Nzc0YmQyYzk4ZGFlMjQ5NDNkMzJjYzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=GQu8tsSU7jOeFN%7EP%7EAJmt4cpFol-fL4aq7CAOrKmk5nopelDJxCwl66V9etIeFtsjMDsSrE6Pl5paJLasnG9-IXObQ6IZrqLMadBr3g-uMKJWCfVC3eN1qTZL8Cm9gCwVAJIWVt5opSC36SNVQJBskS0zTYkLdzvqgsojlXvaTLNuednqR3EYLPL-e%7EWDREIPhnxHVAwtsUIj7yXJthVxOkQ1yPoVBzQIhdwA2tg45H0Xzr9tVnnfp9bmwkRD45-PYDffgA1o7s3bN07jBDyEvWL42uzmqFfMBYwH9zK3dp8PM6MemUoLrCdJXK-EIR3xQX4by1RIfkpLbFN0itQRQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-11-18 11:19:03--  https://cdn-lfs.huggingface.co/repos/30/e3/30e3aca7233f7337633262ff6d59dd98559ecd8982e7419b39752c8d0daae1ca/3bfdde943555c78294626a6ccd40184162d066d39774bd2c98dae24943d32cc3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.ggmlv3.q8_0.bin%3B+filename%3D%22llama-2-7b-chat.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1700565543&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMDU2NTU0M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMC9lMy8zMGUzYWNhNzIzM2Y3MzM3NjMzMjYyZmY2ZDU5ZGQ5ODU1OWVjZDg5ODJlNzQxOWIzOTc1MmM4ZDBkYWFlMWNhLzNiZmRkZTk0MzU1NWM3ODI5NDYyNmE2Y2NkNDAxODQxNjJkMDY2ZDM5Nzc0YmQyYzk4ZGFlMjQ5NDNkMzJjYzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=GQu8tsSU7jOeFN%7EP%7EAJmt4cpFol-fL4aq7CAOrKmk5nopelDJxCwl66V9etIeFtsjMDsSrE6Pl5paJLasnG9-IXObQ6IZrqLMadBr3g-uMKJWCfVC3eN1qTZL8Cm9gCwVAJIWVt5opSC36SNVQJBskS0zTYkLdzvqgsojlXvaTLNuednqR3EYLPL-e%7EWDREIPhnxHVAwtsUIj7yXJthVxOkQ1yPoVBzQIhdwA2tg45H0Xzr9tVnnfp9bmwkRD45-PYDffgA1o7s3bN07jBDyEvWL42uzmqFfMBYwH9zK3dp8PM6MemUoLrCdJXK-EIR3xQX4by1RIfkpLbFN0itQRQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.162.95, 108.157.162.58, 108.157.162.99, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.162.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7160799872 (6.7G) [application/octet-stream]\n",
            "Saving to: ‘llama-2-7b-chat.ggmlv3.q8_0.bin’\n",
            "\n",
            "llama-2-7b-chat.ggm 100%[===================>]   6.67G   141MB/s    in 49s     \n",
            "\n",
            "2023-11-18 11:19:52 (140 MB/s) - ‘llama-2-7b-chat.ggmlv3.q8_0.bin’ saved [7160799872/7160799872]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Llama cpp\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZGxsJoCD7G0",
        "outputId": "5b0c05a2-fca4-43dd-c360-8a1842b11d79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5822257 sha256=88f8ad87eda51ab6386fe1ca2d85c13ff367995da81c8d2326e8a58556e16598\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.1.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import pennylane as qml\n",
        "from pennylane import numpy as np\n",
        "from textblob import TextBlob\n",
        "from llama_cpp import Llama\n",
        "import logging\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "llm = Llama(\n",
        "  model_path=\"llama-2-7b-chat.ggmlv3.q8_0.bin\",\n",
        "  n_gpu_layers=-1,\n",
        "  n_ctx=3900,\n",
        ")\n",
        "# Initialize the quantum device\n",
        "dev = qml.device(\"default.qubit\", wires=4)\n",
        "\n",
        "# Quantum circuit definition\n",
        "@qml.qnode(dev)\n",
        "def quantum_circuit(color_code, amplitude):\n",
        "    r, g, b = [int(color_code[i:i+2], 16) for i in (1, 3, 5)]\n",
        "    r, g, b = r / 255.0, g / 255.0, b / 255.0\n",
        "    qml.RY(r * np.pi, wires=0)\n",
        "    qml.RY(g * np.pi, wires=1)\n",
        "    qml.RY(b * np.pi, wires=2)\n",
        "    qml.RY(amplitude * np.pi, wires=3)\n",
        "    qml.CNOT(wires=[0, 1])\n",
        "    qml.CNOT(wires=[1, 2])\n",
        "    qml.CNOT(wires=[2, 3])\n",
        "    return qml.state()\n",
        "\n",
        "# Sentiment analysis to amplitude mapping\n",
        "def sentiment_to_amplitude(text):\n",
        "    analysis = TextBlob(text)\n",
        "    return (analysis.sentiment.polarity + 1) / 2\n",
        "\n",
        "\n",
        "\n",
        "# Llama2 text generation and color code extraction\n",
        "async def llama_generate_and_colorize(session, prompt, max_tokens=3900, chunk_size=980):\n",
        "    prompt_chunks = [prompt[i:i + chunk_size] for i in range(0, len(prompt), chunk_size)]\n",
        "    generated_text = \"\"\n",
        "    color_code = \"#000000\"\n",
        "    for chunk in prompt_chunks:\n",
        "        output = llm(chunk, max_tokens=min(max_tokens, chunk_size), stop=[\"Q:\", \"\\n\"], echo=True)\n",
        "        text = output.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
        "        generated_text += text\n",
        "        color_code = re.findall(r'#[0-9A-Fa-f]{6}', text)[-1] if re.findall(r'#[0-9A-Fa-f]{6}', text) else color_code\n",
        "    return generated_text, color_code\n",
        "\n",
        "# Function to split the large JSON file into smaller chunks\n",
        "def split_json_file(input_file, max_size_mb=1, output_dir='split_jsons'):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    with open(input_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    current_file_size = 0\n",
        "    current_file_data = []\n",
        "    file_count = 1\n",
        "\n",
        "    for item in data:\n",
        "        item_json = json.dumps(item)\n",
        "        item_size = len(item_json.encode('utf-8'))\n",
        "\n",
        "        if current_file_size + item_size > max_size_mb * 1024 * 1024:\n",
        "            with open(f'{output_dir}/part_{file_count}.json', 'w') as output_file:\n",
        "                json.dump(current_file_data, output_file, indent=4)\n",
        "            current_file_data = [item]\n",
        "            current_file_size = item_size\n",
        "            file_count += 1\n",
        "        else:\n",
        "            current_file_data.append(item)\n",
        "            current_file_size += item_size\n",
        "\n",
        "    if current_file_data:\n",
        "        with open(f'{output_dir}/part_{file_count}.json', 'w') as output_file:\n",
        "            json.dump(current_file_data, output_file, indent=4)\n",
        "\n",
        "    print(f\"Split into {file_count} files.\")\n",
        "\n",
        "# Asynchronous batch sending function\n",
        "async def send_batch_async(batch, batch_number, session, weaviate_url):\n",
        "    start_time = time.time()\n",
        "    tasks = []\n",
        "    for item in batch:\n",
        "        if 'id' in item:\n",
        "            item['message_id'] = item.pop('id')\n",
        "        if 'mapping' in item and isinstance(item['mapping'], dict):\n",
        "            item['mapping'] = json.dumps(item['mapping'])\n",
        "        if 'moderation_results' in item and isinstance(item['moderation_results'], list):\n",
        "            item['moderation_results'] = json.dumps(item['moderation_results'])\n",
        "        url = f\"{weaviate_url}/objects\"\n",
        "        tasks.append(session.post(url, json={\"class\": \"ChatGPTHistory\", \"properties\": item}))\n",
        "    responses = await asyncio.gather(*tasks)\n",
        "    end_time = time.time()\n",
        "\n",
        "# Main asynchronous function\n",
        "async def main():\n",
        "    weaviate_url = \"http://TACOAILINKHERE/1\"\n",
        "    timeout = aiohttp.ClientTimeout(total=9990)\n",
        "    split_json_file('conversations.json')  # Split the large JSON file\n",
        "\n",
        "    max_chars_per_batch = 2000\n",
        "    batch_number = 1\n",
        "\n",
        "    for part_file in os.listdir('split_jsons'):\n",
        "        with open(f'split_jsons/{part_file}', 'r') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        async with aiohttp.ClientSession(timeout=timeout) as session:\n",
        "            while data:\n",
        "                current_batch = []\n",
        "                current_char_count = 0\n",
        "                while current_char_count < max_chars_per_batch and data:\n",
        "                    item = random.choice(data)\n",
        "                    data.remove(item)\n",
        "                    item_json = json.dumps(item)\n",
        "                    item_size = len(item_json)\n",
        "                    if current_char_count + item_size <= max_chars_per_batch:\n",
        "                        current_batch.append(item)\n",
        "                        current_char_count += item_size\n",
        "                if current_batch:\n",
        "                    await send_batch_async(current_batch, batch_number, session, weaviate_url)\n",
        "                    batch_number += 1\n",
        "    print(\"All data sent.\")\n",
        "\n",
        "# Run the main coroutine\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "id": "lf8Cm7f3Dmj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "CclLm3O0KAH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6pJRtagKDmo7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}